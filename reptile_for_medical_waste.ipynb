{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geoffrey256/meta-learning-final-year/blob/main/reptile_for_medical_waste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCM8E_xnUo0X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n99O6ZtWVLhg",
        "outputId": "4ca2c2a2-8478-499d-af72-f2cc3fc2c1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Classes: ['chemical', 'general waste', 'infectious waste', 'pharmacutical', 'sharps']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Dataset base path\n",
        "base_path = \"/content/drive/MyDrive/Year3 /Datasets/medical_dataset/Medical_Waste_Dataset\"\n",
        "\n",
        "# Training directory\n",
        "train_path = os.path.join(base_path, \"train\")\n",
        "\n",
        "# List class folders inside train\n",
        "class_names = sorted(os.listdir(train_path))\n",
        "print(\"Available Classes:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMCbkgPKXKsk",
        "outputId": "eea7af24-4b5b-4b10-8e93-6660aa4fe40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image sizes (height x width):\n",
            "(224, 224): 50 images\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from collections import Counter\n",
        "\n",
        "# Dictionary to count image sizes\n",
        "size_counter = Counter()\n",
        "\n",
        "# Loop through class folders and sample sizes\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(train_path, class_name)\n",
        "    files_checked = 0\n",
        "\n",
        "    for img_file in os.listdir(class_dir):\n",
        "        img_path = os.path.join(class_dir, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is not None:\n",
        "            size_counter[img.shape[:2]] += 1  # (height, width)\n",
        "            files_checked += 1\n",
        "\n",
        "        if files_checked >= 10:  # Check only first 10 images per class\n",
        "            break\n",
        "\n",
        "print(\"Image sizes (height x width):\")\n",
        "for size, count in size_counter.items():\n",
        "    print(f\"{size}: {count} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrMU5RKvX1i_",
        "outputId": "9976e9c0-3652-4546-b8b2-1d14e3a9da73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading chemical: 100%|██████████| 24/24 [00:02<00:00,  8.31it/s]\n",
            "Loading general waste: 100%|██████████| 76/76 [00:14<00:00,  5.42it/s]\n",
            "Loading infectious waste: 100%|██████████| 144/144 [00:00<00:00, 168.48it/s]\n",
            "Loading pharmacutical: 100%|██████████| 160/160 [00:00<00:00, 170.91it/s]\n",
            "Loading sharps: 100%|██████████| 364/364 [00:02<00:00, 160.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chemical: 24 images\n",
            "general waste: 76 images\n",
            "infectious waste: 144 images\n",
            "pharmacutical: 160 images\n",
            "sharps: 364 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "IMG_SIZE = 84  # target image size\n",
        "dataset = {}   # class-wise dictionary of images\n",
        "\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(train_path, class_name)\n",
        "    images = []\n",
        "\n",
        "    for img_file in tqdm(os.listdir(class_dir), desc=f\"Loading {class_name}\"):\n",
        "        img_path = os.path.join(class_dir, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
        "            images.append(img)\n",
        "\n",
        "    dataset[class_name] = images\n",
        "\n",
        "# Print number of images per class\n",
        "for cls in dataset:\n",
        "    print(f\"{cls}: {len(dataset[cls])} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5iFtUjlZfWo"
      },
      "source": [
        "**Build a Task Sampler (Support & Query Set Generator)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIQpWAm6YpTI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def create_episode(dataset, n_way=5, k_shot=1, q_query=5):\n",
        "    \"\"\"\n",
        "    Creates one episode with support and query sets.\n",
        "\n",
        "    Args:\n",
        "        dataset (dict): Class-wise image dictionary.\n",
        "        n_way (int): Number of classes per task.\n",
        "        k_shot (int): Number of support images per class.\n",
        "        q_query (int): Number of query images per class.\n",
        "\n",
        "    Returns:\n",
        "        support_x, support_y, query_x, query_y: Lists of images and labels\n",
        "    \"\"\"\n",
        "    selected_classes = random.sample(list(dataset.keys()), n_way)\n",
        "    support_x, support_y = [], []\n",
        "    query_x, query_y = [], []\n",
        "\n",
        "    for label_idx, class_name in enumerate(selected_classes):\n",
        "        images = random.sample(dataset[class_name], k_shot + q_query)\n",
        "        support_imgs = images[:k_shot]\n",
        "        query_imgs = images[k_shot:]\n",
        "\n",
        "        support_x.extend(support_imgs)\n",
        "        support_y.extend([label_idx] * k_shot)\n",
        "\n",
        "        query_x.extend(query_imgs)\n",
        "        query_y.extend([label_idx] * q_query)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    support_x = np.array(support_x)\n",
        "    support_y = np.array(support_y)\n",
        "    query_x = np.array(query_x)\n",
        "    query_y = np.array(query_y)\n",
        "\n",
        "    return support_x, support_y, query_x, query_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgenVh20Z-h7"
      },
      "source": [
        "**Build the Reptile Model Architecture & Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_On2FSakZ0dZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def build_cnn_model(input_shape=(84, 84, 3), num_classes=5):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nD4hg7YaYDV"
      },
      "source": [
        "**Compile and test CNN model on a sample episode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyZGxIjMaPCR",
        "outputId": "e49c14f5-55d5-4104-a817-714e83dae265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:708: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.2000 - loss: 1.6013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fed8a36cb50>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Build CNN model with explicit Input layer to avoid warnings\n",
        "def build_cnn_model(input_shape=(84, 84, 3), num_classes=5):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=input_shape),  # Explicit Input layer\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Step 2: Compile the model\n",
        "# model = build_cnn_model(input_shape=(84, 84, 3), num_classes=5)\n",
        "# model.compile(\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "#     loss='sparse_categorical_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# Create and compile model\n",
        "model = build_cnn_model(input_shape=(84, 84, 3),num_classes=5)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Initialize meta weights\n",
        "meta_weights = model.get_weights()\n",
        "\n",
        "\n",
        "# Step 3: Generate one episode (you should have this function from before)\n",
        "support_x, support_y, query_x, query_y = create_episode(dataset, n_way=5, k_shot=1, q_query=5)\n",
        "\n",
        "# Step 4: Train the model on support set for 1 epoch (quick test)\n",
        "model.fit(support_x, support_y, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DEOcORQb6So"
      },
      "source": [
        "**Code Weight**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hnw7ldEaf8V"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Helper: Get weights (deep copy)\n",
        "def get_weights(model):\n",
        "    return [w.numpy() for w in model.trainable_weights]\n",
        "\n",
        "# Helper: Set weights\n",
        "def set_weights(model, weights):\n",
        "    for var, w in zip(model.trainable_weights, weights):\n",
        "        var.assign(w)\n",
        "\n",
        "# Helper: Interpolate weights between initial and post-task weights\n",
        "def interpolate_weights(old_weights, new_weights, epsilon=0.1):\n",
        "    return [old + epsilon * (new - old) for old, new in zip(old_weights, new_weights)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQIHe4dRb2iF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Configs\n",
        "num_iterations = 1000        # Number of meta-training steps\n",
        "inner_epochs = 1             # How long we train on each task (inner loop)\n",
        "inner_batch_size = 8         # Batch size per task\n",
        "epsilon = 0.1                # Step size for Reptile update\n",
        "n_way = 3                    # Number of classes per task\n",
        "k_shot = 5                   # Number of images per class (support set)\n",
        "query_shot = 2               # Number of images per class (query set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZfAUpejezXu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "def sample_task(dataset_path, n_way=3, k_shot=5, query_shot=2, img_size=(84, 84)):\n",
        "    class_names = random.sample(os.listdir(dataset_path), n_way)\n",
        "    support_images = []\n",
        "    support_labels = []\n",
        "    query_images = []\n",
        "    query_labels = []\n",
        "\n",
        "    label_map = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = os.listdir(class_dir)\n",
        "        selected = random.sample(image_files, k_shot + query_shot)\n",
        "\n",
        "        support = selected[:k_shot]\n",
        "        query = selected[k_shot:]\n",
        "\n",
        "        for img_name in support:\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            img = tf.keras.utils.load_img(img_path, target_size=img_size)\n",
        "            img = tf.keras.utils.img_to_array(img) / 255.0\n",
        "            support_images.append(img)\n",
        "            support_labels.append(label_map[class_name])\n",
        "\n",
        "        for img_name in query:\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            img = tf.keras.utils.load_img(img_path, target_size=img_size)\n",
        "            img = tf.keras.utils.img_to_array(img) / 255.0\n",
        "            query_images.append(img)\n",
        "            query_labels.append(label_map[class_name])\n",
        "\n",
        "    support_dataset = tf.data.Dataset.from_tensor_slices((support_images, support_labels))\n",
        "    query_dataset = tf.data.Dataset.from_tensor_slices((query_images, query_labels))\n",
        "\n",
        "    support_dataset = support_dataset.shuffle(100).batch(inner_batch_size)\n",
        "    query_dataset = query_dataset.shuffle(100).batch(inner_batch_size)\n",
        "\n",
        "    return support_dataset, query_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJVhmWpre9iS",
        "outputId": "d0c1d9aa-57c5-4123-d021-d2b64b0c69ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/1000 done.\n",
            "Iteration 100/1000 done.\n",
            "Iteration 200/1000 done.\n",
            "Iteration 300/1000 done.\n",
            "Iteration 400/1000 done.\n",
            "Iteration 500/1000 done.\n",
            "Iteration 600/1000 done.\n",
            "Iteration 700/1000 done.\n",
            "Iteration 800/1000 done.\n",
            "Iteration 900/1000 done.\n",
            "Iteration 1000/1000 done.\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Main Reptile training loop\n",
        "for iteration in range(num_iterations):\n",
        "    # Step 1: Sample a task (N-way K-shot)\n",
        "    support_ds, _ = sample_task(\n",
        "        dataset_path=\"/content/drive/MyDrive/Year3 /Datasets/medical_dataset/Medical_Waste_Dataset/train\",\n",
        "        n_way=n_way, k_shot=k_shot, query_shot=query_shot\n",
        "    )\n",
        "\n",
        "    # Step 2: Save the initial weights\n",
        "    old_weights = get_weights(model)\n",
        "\n",
        "    # Step 3: Inner loop training on the support set\n",
        "    for epoch in range(inner_epochs):\n",
        "        for x_batch, y_batch in support_ds:\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x_batch, training=True)\n",
        "                loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, logits, from_logits=True)\n",
        "                loss = tf.reduce_mean(loss)\n",
        "            grads = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    # Step 4: Get new weights after the inner update\n",
        "    new_weights = get_weights(model)\n",
        "\n",
        "    # Step 5: Reptile update: Move original weights toward new ones\n",
        "    updated_weights = interpolate_weights(old_weights, new_weights, epsilon=epsilon)\n",
        "    set_weights(model, updated_weights)\n",
        "\n",
        "    # Logging progress\n",
        "    if (iteration + 1) % 100 == 0 or iteration == 0:\n",
        "        print(f\"Iteration {iteration+1}/{num_iterations} done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkkuS15HfpdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95dfb90a-2ae1-4ee4-fb52-a3957c065a57"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Sample larger than population or is negative",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-3825538971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Example call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m evaluate_reptile_task(\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Year3 /Datasets/medical_dataset/Medical_Waste_Dataset/test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12-3825538971.py\u001b[0m in \u001b[0;36mevaluate_reptile_task\u001b[0;34m(model, dataset_path, n_way, k_shot, query_shot, img_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_reptile_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_way\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Step 1: Sample task from validation or test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     support_ds, query_ds = sample_task(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_way\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_way\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-10-3736926645.py\u001b[0m in \u001b[0;36msample_task\u001b[0;34m(dataset_path, n_way, k_shot, query_shot, img_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclass_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_shot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery_shot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk_shot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
          ]
        }
      ],
      "source": [
        "def evaluate_reptile_task(model, dataset_path, n_way=3, k_shot=5, query_shot=5, img_size=(84, 84)):\n",
        "    # Step 1: Sample task from validation or test set\n",
        "    support_ds, query_ds = sample_task(\n",
        "        dataset_path, n_way=n_way, k_shot=k_shot, query_shot=query_shot, img_size=img_size\n",
        "    )\n",
        "\n",
        "    # Step 2: Create a copy of the model for adaptation\n",
        "    eval_model = tf.keras.models.clone_model(model)\n",
        "    eval_model.set_weights(get_weights(model))\n",
        "\n",
        "    # Step 3: Fine-tune the copied model on support set\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    for x_batch, y_batch in support_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = eval_model(x_batch, training=True)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, logits, from_logits=True)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "        grads = tape.gradient(loss, eval_model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, eval_model.trainable_weights))\n",
        "\n",
        "    # Step 4: Evaluate on query set\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for x_batch, y_batch in query_ds:\n",
        "        logits = eval_model(x_batch, training=False)\n",
        "        preds = tf.argmax(logits, axis=1)\n",
        "        correct += tf.reduce_sum(tf.cast(preds == tf.cast(y_batch, tf.int64), tf.int32)).numpy()\n",
        "        total += y_batch.shape[0]\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Meta-Test Accuracy on New Task: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Example call\n",
        "evaluate_reptile_task(\n",
        "    model,\n",
        "    dataset_path=\"/content/drive/MyDrive/Year3 /Datasets/medical_dataset/Medical_Waste_Dataset/test\",\n",
        "    n_way=3, k_shot=5, query_shot=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV3ZMPFvN-pX"
      },
      "outputs": [],
      "source": [
        "# def create_base_model(num_classes):\n",
        "#     model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.Input(shape=(84, 84, 3)),\n",
        "#         tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "#         tf.keras.layers.MaxPooling2D(),\n",
        "#         tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "#         tf.keras.layers.MaxPooling2D(),\n",
        "#         tf.keras.layers.Flatten(),\n",
        "#         tf.keras.layers.Dense(128, activation='relu'),\n",
        "#         tf.keras.layers.Dense(num_classes)  # No softmax: logits used for sparse_categorical_crossentropy with from_logits=True\n",
        "#     ])\n",
        "#     return model\n",
        "\n",
        "def create_base_model(num_classes=5):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(224, 224, 3)),\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_classes)  # no softmax if using from_logits=True\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FNFgo65NLjx"
      },
      "source": [
        "**Modify Reptile Training Loop to Log Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP2VkMlZZ3Uj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define dataset path\n",
        "train_ds = \"/content/drive/MyDrive/Year3 /Datasets/medical_dataset/Medical_Waste_Dataset/train\"\n",
        "\n",
        "def sample_task(dataset_path, num_classes=5, shots=5, img_size=(224, 224), batch_size=16):\n",
        "    class_names = os.listdir(dataset_path)\n",
        "    selected_classes = random.sample(class_names, num_classes)\n",
        "\n",
        "    support_images = []\n",
        "    support_labels = []\n",
        "    query_images = []\n",
        "    query_labels = []\n",
        "\n",
        "    label_map = {cls_name: idx for idx, cls_name in enumerate(selected_classes)}\n",
        "\n",
        "    for cls_name in selected_classes:\n",
        "        cls_path = os.path.join(dataset_path, cls_name)\n",
        "        image_paths = [os.path.join(cls_path, fname) for fname in os.listdir(cls_path)]\n",
        "        selected_images = random.sample(image_paths, shots + 1)  # 1 for query\n",
        "\n",
        "        for i, img_path in enumerate(selected_images):\n",
        "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
        "            img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "            label = label_map[cls_name]\n",
        "\n",
        "            if i < shots:\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label)\n",
        "            else:\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label)\n",
        "\n",
        "    support_ds = tf.data.Dataset.from_tensor_slices((support_images, support_labels)).batch(batch_size)\n",
        "    query_ds = tf.data.Dataset.from_tensor_slices((query_images, query_labels)).batch(batch_size)\n",
        "\n",
        "    return support_ds, query_ds\n",
        "\n",
        "\n",
        "\n",
        "model = create_base_model(num_classes=5)\n",
        "meta_weights = model.get_weights()\n",
        "\n",
        "support_ds, query_ds = sample_task(train_ds, num_classes=5, shots=5)\n",
        "model.set_weights(meta_weights)\n",
        "model.fit(support_ds, epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "# # Run sample task\n",
        "# support_ds, query_ds = sample_task(train_ds, num_classes=5, shots=5)\n",
        "# model.set_weights(meta_weights)\n",
        "# model.fit(support_ds, epochs=1, verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf9qoUwDRai9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "def sample_task(dataset_path, num_classes=5, shots=5, img_size=(224, 224), batch_size=16):\n",
        "    class_names = os.listdir(dataset_path)\n",
        "    selected_classes = random.sample(class_names, num_classes)\n",
        "\n",
        "    support_images = []\n",
        "    support_labels = []\n",
        "    query_images = []\n",
        "    query_labels = []\n",
        "\n",
        "    label_map = {cls_name: idx for idx, cls_name in enumerate(selected_classes)}\n",
        "\n",
        "    for cls_name in selected_classes:\n",
        "        cls_path = os.path.join(dataset_path, cls_name)\n",
        "        image_paths = [os.path.join(cls_path, fname) for fname in os.listdir(cls_path)]\n",
        "        selected_images = random.sample(image_paths, shots + 1)  # 1 for query\n",
        "\n",
        "        for i, img_path in enumerate(selected_images):\n",
        "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
        "            img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "            label = label_map[cls_name]\n",
        "\n",
        "            if i < shots:\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label)\n",
        "            else:\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label)\n",
        "\n",
        "    support_ds = tf.data.Dataset.from_tensor_slices((support_images, support_labels)).batch(batch_size)\n",
        "    query_ds = tf.data.Dataset.from_tensor_slices((query_images, query_labels)).batch(batch_size)\n",
        "\n",
        "    return support_ds, query_ds\n",
        "\n",
        "support_ds, query_ds = sample_task(train_ds, num_classes=5, shots=5)\n",
        "model.set_weights(meta_weights)\n",
        "model.fit(support_ds, epochs=1, verbose=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPBoErp3RjkJ"
      },
      "outputs": [],
      "source": [
        "# Load final weights into model\n",
        "model.set_weights(meta_weights)\n",
        "\n",
        "# Evaluate on 10 new few-shot tasks\n",
        "eval_accuracies = []\n",
        "\n",
        "for _ in range(100):\n",
        "    support_ds, query_ds = sample_task(train_path, num_classes=5, shots=5)\n",
        "    model.set_weights(meta_weights)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(support_ds, epochs=1, verbose=0)\n",
        "    meta_weights = model.get_weights()  # Reptile update\n",
        "\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet',\n",
        "                                               pooling='avg')\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Freeze base\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for _ in range(100):\n",
        "#     support_ds, query_ds = sample_task(train_ds, num_classes=5, shots=5)\n",
        "#     model.fit(support_ds, epochs=1, verbose=0)\n",
        "\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "\n",
        "#     for x_batch, y_batch in query_ds:\n",
        "#         logits = model(x_batch, training=False)\n",
        "#         preds = tf.argmax(logits, axis=1)\n",
        "#         y_batch = tf.cast(y_batch, tf.int64)\n",
        "#         correct += tf.reduce_sum(tf.cast(preds == y_batch, tf.int32)).numpy()\n",
        "#         total += y_batch.shape[0]\n",
        "\n",
        "#     eval_accuracy = correct / total\n",
        "#     eval_accuracies.append(eval_accuracy)\n",
        "\n",
        "# # Print average accuracy over evaluation tasks\n",
        "# avg_eval_accuracy = sum(eval_accuracies) / len(eval_accuracies)\n",
        "# print(f\"Average Evaluation Accuracy over 10 tasks: {avg_eval_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "p3Ejs7bZW-8W",
        "outputId": "c48ec718-2412-47ae-9420-83f224d8cf0e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2000\n",
            "Recall (macro): 0.2000\n",
            "F1-score (macro): 0.0667\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.20      1.00      0.33         1\n",
            "\n",
            "    accuracy                           0.20         5\n",
            "   macro avg       0.04      0.20      0.07         5\n",
            "weighted avg       0.04      0.20      0.07         5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "\n",
        "for x_batch, y_batch in query_ds:\n",
        "    logits = model(x_batch, training=False)\n",
        "    preds = tf.argmax(logits, axis=1)\n",
        "\n",
        "    y_true_all.extend(y_batch.numpy())\n",
        "    y_pred_all.extend(preds.numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluation Metrics\n",
        "accuracy = accuracy_score(y_true_all, y_pred_all)\n",
        "recall = recall_score(y_true_all, y_pred_all, average='macro')\n",
        "f1 = f1_score(y_true_all, y_pred_all, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall (macro): {recall:.4f}\")\n",
        "print(f\"F1-score (macro): {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_true_all, y_pred_all))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmozbk4kRjgB",
        "outputId": "7114867a-278c-4b74-e269-035716c98bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "model.save(\"model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3JlolAGRjcz"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save weights as list\n",
        "weights = model.get_weights()\n",
        "with open(\"model_weights.pkl\", \"wb\") as f:\n",
        "    pickle.dump(weights, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iePQ6XERjaH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "wijVtAum6mEA",
        "outputId": "28e8e02e-014e-4304-da06-932aec2b58ff"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "sample_task() got an unexpected keyword argument 'num_classes'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e6b773bb41d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Sample task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msupport_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Train on support set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sample_task() got an unexpected keyword argument 'num_classes'"
          ]
        }
      ],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import tensorflow as tf\n",
        "\n",
        "# train_ds = \"/content/drive/MyDrive/Year3 /Datasets/medical_dataset/Medical_Waste_Dataset/train\"\n",
        "\n",
        "# # Re-initialize the base model (if not already)\n",
        "# model = create_base_model(num_classes=5)\n",
        "\n",
        "# # Initialize meta weights from the base model\n",
        "# meta_weights = model.get_weights()\n",
        "\n",
        "# # Set your meta-learning rate\n",
        "# meta_step_size = 0.1\n",
        "\n",
        "# # Logging lists\n",
        "# losses = []\n",
        "# accuracies = []\n",
        "\n",
        "# # Reptile training loop\n",
        "# for iteration in range(1000):\n",
        "#     model.set_weights(meta_weights)\n",
        "\n",
        "#     # Sample task\n",
        "#     support_ds, query_ds = sample_task(train_ds, num_classes=5, shots=5)\n",
        "\n",
        "#     # Train on support set\n",
        "#     model.fit(support_ds, epochs=1, verbose=0)\n",
        "\n",
        "#     # Evaluate on query set\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     loss_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "#     for x_batch, y_batch in query_ds:\n",
        "#         logits = model(x_batch, training=False)\n",
        "#         loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "#             y_batch, logits, from_logits=True\n",
        "#         )\n",
        "#         preds = tf.argmax(logits, axis=1)\n",
        "#         y_batch = tf.cast(y_batch, tf.int64)\n",
        "#         correct += tf.reduce_sum(tf.cast(preds == y_batch, tf.int32)).numpy()\n",
        "#         total += y_batch.shape[0]\n",
        "#         loss_metric.update_state(loss)\n",
        "\n",
        "#     accuracy = correct / total\n",
        "#     losses.append(loss_metric.result().numpy())\n",
        "#     accuracies.append(accuracy)\n",
        "\n",
        "#     # Meta-update: update meta-weights\n",
        "#     new_weights = model.get_weights()\n",
        "#     meta_weights = [(1 - meta_step_size) * old + meta_step_size * new\n",
        "#                     for old, new in zip(meta_weights, new_weights)]\n",
        "\n",
        "#     if (iteration + 1) % 100 == 0:\n",
        "#         print(f\"Iteration {iteration + 1}/1000 - Loss: {loss_metric.result():.4f} - Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmwNDscLPPxg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**version 2**"
      ],
      "metadata": {
        "id": "kofDdu-2QyBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F82t8QmrPPu6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (224, 224)\n",
        "train_path = \"/content/drive/MyDrive/Datasets/Extracted/Pharmaceutical_and_Biomedical_Waste\"\n",
        "num_meta_iterations = 100  # ↑ Increase this\n",
        "num_classes = 5\n",
        "shots = 5\n"
      ],
      "metadata": {
        "id": "ssLbV6YgwUu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_classes=5):\n",
        "    base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg')\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kC1f662dwVtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_task(data_path, num_classes=5, shots=5):\n",
        "    class_names = os.listdir(data_path)\n",
        "    selected_classes = random.sample(class_names, num_classes)\n",
        "\n",
        "    support_images, support_labels = [], []\n",
        "    query_images, query_labels = [], []\n",
        "\n",
        "    for idx, cls in enumerate(selected_classes):\n",
        "        class_path = os.path.join(data_path, cls)\n",
        "        all_images = os.listdir(class_path)\n",
        "        selected_images = random.sample(all_images, shots * 2)  # 2x shots (support + query)\n",
        "\n",
        "        for i in range(shots):\n",
        "            img = image.load_img(os.path.join(class_path, selected_images[i]), target_size=img_size)\n",
        "            img = image.img_to_array(img) / 255.0\n",
        "            img = tf.image.random_flip_left_right(img)\n",
        "            img = tf.image.random_brightness(img, max_delta=0.1)\n",
        "            support_images.append(img)\n",
        "            support_labels.append(idx)\n",
        "\n",
        "        for i in range(shots, shots * 2):\n",
        "            img = image.load_img(os.path.join(class_path, selected_images[i]), target_size=img_size)\n",
        "            img = image.img_to_array(img) / 255.0\n",
        "            img = tf.image.random_flip_left_right(img)\n",
        "            img = tf.image.random_brightness(img, max_delta=0.1)\n",
        "            query_images.append(img)\n",
        "            query_labels.append(idx)\n",
        "\n",
        "    support_ds = tf.data.Dataset.from_tensor_slices((np.array(support_images), np.array(support_labels))).batch(5)\n",
        "    query_ds = tf.data.Dataset.from_tensor_slices((np.array(query_images), np.array(query_labels))).batch(5)\n",
        "\n",
        "    return support_ds, query_ds\n"
      ],
      "metadata": {
        "id": "jrh5W5w5wZyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and build it with dummy input\n",
        "model = build_model(num_classes)\n",
        "model.build(input_shape=(None, 224, 224, 3))\n",
        "meta_weights = model.get_weights()\n",
        "\n",
        "# Meta-training loop\n",
        "for iteration in range(num_meta_iterations):\n",
        "    support_ds, query_ds = sample_task(train_path, num_classes=num_classes, shots=shots)\n",
        "\n",
        "    # Set model weights and compile for each task\n",
        "    model.set_weights(meta_weights)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train on the support set\n",
        "    model.fit(support_ds, epochs=1, verbose=0)\n",
        "\n",
        "    # Get new weights after training on this task\n",
        "    new_weights = model.get_weights()\n",
        "\n",
        "    # Reptile update: blend old and new weights\n",
        "    updated_weights = [meta_w + 0.1 * (new_w - meta_w) for meta_w, new_w in zip(meta_weights, new_weights)]\n",
        "    meta_weights = updated_weights\n",
        "\n",
        "    if iteration % 10 == 0:\n",
        "        print(f\"Completed {iteration}/{num_meta_iterations} iterations\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "A1fU5RayweDz",
        "outputId": "d6d51cb6-91f5-401c-a9eb-7d0ec9896272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Sample larger than population or is negative",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-cc468e527dcc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Meta-training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_meta_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msupport_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Set model weights and compile for each task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-b0e83ecf0d02>\u001b[0m in \u001b[0;36msample_task\u001b[0;34m(data_path, num_classes, shots)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mselected_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msupport_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XwEJ-flwrap"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWxJGFQ8CGqaJAQF5jXExZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}